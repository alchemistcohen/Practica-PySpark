{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOc0upE6mTr8Cfuee7GuI+G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alchemistcohen/Practica-PySpark/blob/main/Practica_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practica PySpark"
      ],
      "metadata": {
        "id": "b5XJN7bI3I5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este proyecto práctico se centra en la transformación e integración de datos mediante PySpark. Trabajamos con dos conjuntos de datos, realizamos diversas transformaciones como agregar, renombrar y eliminar columnas innecesarias, unir dataframes y, finalmente, escribimos los resultados tanto en un almacén de datos Hive como en un sistema de archivos HDFS."
      ],
      "metadata": {
        "id": "DQ8htEwb3PvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalar Librerias requeridas"
      ],
      "metadata": {
        "id": "hCzaZ-8D3nEy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZJWGpio20y3",
        "outputId": "75ae5d74-c299-4a2f-ca78-3b52a52ae0b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=5788899f0adc6815ddc6515c4a5f051b2834e326e8d538a71a3912951450eb22\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/46/3b/e29ffbe4ebe614ff224bad40fc6a5773a67a163251585a13a9\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, findspark\n",
            "Successfully installed findspark-2.0.1 wget-3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install wget pyspark  findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-vlHOBe5QSN",
        "outputId": "9feba20c-199d-484d-c174-e3aab4729fe6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  ca-certificates-java fonts-dejavu-core fonts-dejavu-extra java-common\n",
            "  libatk-wrapper-java libatk-wrapper-java-jni libpcsclite1 libxt-dev libxtst6\n",
            "  libxxf86dga1 openjdk-11-jdk-headless openjdk-11-jre openjdk-11-jre-headless\n",
            "  x11-utils\n",
            "Suggested packages:\n",
            "  default-jre pcscd libxt-doc openjdk-11-demo openjdk-11-source visualvm\n",
            "  libnss-mdns fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  | fonts-wqy-zenhei fonts-indic mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  ca-certificates-java fonts-dejavu-core fonts-dejavu-extra java-common\n",
            "  libatk-wrapper-java libatk-wrapper-java-jni libpcsclite1 libxt-dev libxtst6\n",
            "  libxxf86dga1 openjdk-11-jdk openjdk-11-jdk-headless openjdk-11-jre\n",
            "  openjdk-11-jre-headless x11-utils\n",
            "0 upgraded, 15 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 122 MB of archives.\n",
            "After this operation, 274 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 java-common all 0.72build2 [6,782 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpcsclite1 amd64 1.9.5-3ubuntu1 [19.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre-headless amd64 11.0.28+6-1ubuntu1~22.04.1 [42.6 MB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ca-certificates-java all 20190909ubuntu1.2 [12.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.28+6-1ubuntu1~22.04.1 [214 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk-headless amd64 11.0.28+6-1ubuntu1~22.04.1 [73.6 MB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk amd64 11.0.28+6-1ubuntu1~22.04.1 [1,342 kB]\n",
            "Fetched 122 MB in 9s (14.0 MB/s)\n",
            "Selecting previously unselected package java-common.\n",
            "(Reading database ... 125082 files and directories currently installed.)\n",
            "Preparing to unpack .../00-java-common_0.72build2_all.deb ...\n",
            "Unpacking java-common (0.72build2) ...\n",
            "Selecting previously unselected package libpcsclite1:amd64.\n",
            "Preparing to unpack .../01-libpcsclite1_1.9.5-3ubuntu1_amd64.deb ...\n",
            "Unpacking libpcsclite1:amd64 (1.9.5-3ubuntu1) ...\n",
            "Selecting previously unselected package openjdk-11-jre-headless:amd64.\n",
            "Preparing to unpack .../02-openjdk-11-jre-headless_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package ca-certificates-java.\n",
            "Preparing to unpack .../03-ca-certificates-java_20190909ubuntu1.2_all.deb ...\n",
            "Unpacking ca-certificates-java (20190909ubuntu1.2) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../04-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../05-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../06-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../07-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../08-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../09-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../10-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../11-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../12-openjdk-11-jre_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-11-jdk-headless:amd64.\n",
            "Preparing to unpack .../13-openjdk-11-jdk-headless_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-11-jdk:amd64.\n",
            "Preparing to unpack .../14-openjdk-11-jdk_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Setting up java-common (0.72build2) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up libpcsclite1:amd64 (1.9.5-3ubuntu1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\n",
            "Setting up openjdk-11-jre:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Setting up openjdk-11-jdk-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jar to provide /usr/bin/jar (jar) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javap to provide /usr/bin/javap (javap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdeprscan to provide /usr/bin/jdeprscan (jdeprscan) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jfr to provide /usr/bin/jfr (jfr) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jimage to provide /usr/bin/jimage (jimage) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jlink to provide /usr/bin/jlink (jlink) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jmod to provide /usr/bin/jmod (jmod) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jshell to provide /usr/bin/jshell (jshell) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmic to provide /usr/bin/rmic (rmic) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jaotc to provide /usr/bin/jaotc (jaotc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jhsdb to provide /usr/bin/jhsdb (jhsdb) in auto mode\n",
            "Setting up openjdk-11-jdk:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Setting up ca-certificates-java (20190909ubuntu1.2) ...\n",
            "head: cannot open '/etc/ssl/certs/java/cacerts' for reading: No such file or directory\n",
            "Adding debian:Amazon_Root_CA_2.pem\n",
            "Adding debian:TeliaSonera_Root_CA_v1.pem\n",
            "Adding debian:ePKI_Root_Certification_Authority.pem\n",
            "Adding debian:AffirmTrust_Premium.pem\n",
            "Adding debian:DigiCert_High_Assurance_EV_Root_CA.pem\n",
            "Adding debian:Actalis_Authentication_Root_CA.pem\n",
            "Adding debian:QuoVadis_Root_CA_2_G3.pem\n",
            "Adding debian:UCA_Extended_Validation_Root.pem\n",
            "Adding debian:GTS_Root_R4.pem\n",
            "Adding debian:SSL.com_EV_Root_Certification_Authority_RSA_R2.pem\n",
            "Adding debian:Entrust_Root_Certification_Authority.pem\n",
            "Adding debian:GlobalSign_ECC_Root_CA_-_R4.pem\n",
            "Adding debian:Hellenic_Academic_and_Research_Institutions_ECC_RootCA_2015.pem\n",
            "Adding debian:Go_Daddy_Class_2_CA.pem\n",
            "Adding debian:Hellenic_Academic_and_Research_Institutions_RootCA_2015.pem\n",
            "Adding debian:Microsoft_RSA_Root_Certificate_Authority_2017.pem\n",
            "Adding debian:Autoridad_de_Certificacion_Firmaprofesional_CIF_A62634068.pem\n",
            "Adding debian:USERTrust_ECC_Certification_Authority.pem\n",
            "Adding debian:XRamp_Global_CA_Root.pem\n",
            "Adding debian:DigiCert_Assured_ID_Root_CA.pem\n",
            "Adding debian:TunTrust_Root_CA.pem\n",
            "Adding debian:ISRG_Root_X2.pem\n",
            "Adding debian:Entrust_Root_Certification_Authority_-_G4.pem\n",
            "Adding debian:QuoVadis_Root_CA_2.pem\n",
            "Adding debian:DigiCert_Global_Root_CA.pem\n",
            "Adding debian:Starfield_Root_Certificate_Authority_-_G2.pem\n",
            "Adding debian:Entrust_Root_Certification_Authority_-_EC1.pem\n",
            "Adding debian:emSign_Root_CA_-_C1.pem\n",
            "Adding debian:Comodo_AAA_Services_root.pem\n",
            "Adding debian:TUBITAK_Kamu_SM_SSL_Kok_Sertifikasi_-_Surum_1.pem\n",
            "Adding debian:GlobalSign_Root_E46.pem\n",
            "Adding debian:QuoVadis_Root_CA_1_G3.pem\n",
            "Adding debian:SecureTrust_CA.pem\n",
            "Adding debian:Telia_Root_CA_v2.pem\n",
            "Adding debian:D-TRUST_EV_Root_CA_1_2020.pem\n",
            "Adding debian:D-TRUST_Root_Class_3_CA_2_2009.pem\n",
            "Adding debian:Security_Communication_ECC_RootCA1.pem\n",
            "Adding debian:ISRG_Root_X1.pem\n",
            "Adding debian:Izenpe.com.pem\n",
            "Adding debian:Trustwave_Global_Certification_Authority.pem\n",
            "Adding debian:Certum_Trusted_Network_CA_2.pem\n",
            "Adding debian:Amazon_Root_CA_4.pem\n",
            "Adding debian:AffirmTrust_Commercial.pem\n",
            "Adding debian:vTrus_Root_CA.pem\n",
            "Adding debian:T-TeleSec_GlobalRoot_Class_2.pem\n",
            "Adding debian:T-TeleSec_GlobalRoot_Class_3.pem\n",
            "Adding debian:CFCA_EV_ROOT.pem\n",
            "Adding debian:COMODO_Certification_Authority.pem\n",
            "Adding debian:DigiCert_Assured_ID_Root_G3.pem\n",
            "Adding debian:COMODO_ECC_Certification_Authority.pem\n",
            "Adding debian:Buypass_Class_3_Root_CA.pem\n",
            "Adding debian:Go_Daddy_Root_Certificate_Authority_-_G2.pem\n",
            "Adding debian:SecureSign_RootCA11.pem\n",
            "Adding debian:Security_Communication_Root_CA.pem\n",
            "Adding debian:Certum_Trusted_Root_CA.pem\n",
            "Adding debian:GTS_Root_R2.pem\n",
            "Adding debian:IdenTrust_Public_Sector_Root_CA_1.pem\n",
            "Adding debian:Secure_Global_CA.pem\n",
            "Adding debian:SwissSign_Gold_CA_-_G2.pem\n",
            "Adding debian:SSL.com_EV_Root_Certification_Authority_ECC.pem\n",
            "Adding debian:AC_RAIZ_FNMT-RCM.pem\n",
            "Adding debian:Hongkong_Post_Root_CA_3.pem\n",
            "Adding debian:HiPKI_Root_CA_-_G1.pem\n",
            "Adding debian:SSL.com_Root_Certification_Authority_RSA.pem\n",
            "Adding debian:Certum_Trusted_Network_CA.pem\n",
            "Adding debian:emSign_ECC_Root_CA_-_C3.pem\n",
            "Adding debian:NAVER_Global_Root_Certification_Authority.pem\n",
            "Adding debian:Amazon_Root_CA_1.pem\n",
            "Adding debian:TWCA_Root_Certification_Authority.pem\n",
            "Adding debian:QuoVadis_Root_CA_3_G3.pem\n",
            "Adding debian:e-Szigno_Root_CA_2017.pem\n",
            "Adding debian:Atos_TrustedRoot_2011.pem\n",
            "Adding debian:OISTE_WISeKey_Global_Root_GC_CA.pem\n",
            "Adding debian:Starfield_Services_Root_Certificate_Authority_-_G2.pem\n",
            "Adding debian:Certainly_Root_E1.pem\n",
            "Adding debian:NetLock_Arany_=Class_Gold=_Főtanúsítvány.pem\n",
            "Adding debian:CA_Disig_Root_R2.pem\n",
            "Adding debian:GLOBALTRUST_2020.pem\n",
            "Adding debian:Buypass_Class_2_Root_CA.pem\n",
            "Adding debian:OISTE_WISeKey_Global_Root_GB_CA.pem\n",
            "Adding debian:Trustwave_Global_ECC_P256_Certification_Authority.pem\n",
            "Adding debian:ACCVRAIZ1.pem\n",
            "Adding debian:GDCA_TrustAUTH_R5_ROOT.pem\n",
            "Adding debian:DigiCert_TLS_ECC_P384_Root_G5.pem\n",
            "Adding debian:certSIGN_Root_CA_G2.pem\n",
            "Adding debian:SwissSign_Silver_CA_-_G2.pem\n",
            "Adding debian:COMODO_RSA_Certification_Authority.pem\n",
            "Adding debian:DigiCert_Assured_ID_Root_G2.pem\n",
            "Adding debian:GlobalSign_Root_CA.pem\n",
            "Adding debian:Entrust.net_Premium_2048_Secure_Server_CA.pem\n",
            "Adding debian:TWCA_Global_Root_CA.pem\n",
            "Adding debian:Microsec_e-Szigno_Root_CA_2009.pem\n",
            "Adding debian:USERTrust_RSA_Certification_Authority.pem\n",
            "Adding debian:Certum_EC-384_CA.pem\n",
            "Adding debian:GTS_Root_R3.pem\n",
            "Adding debian:Certigna_Root_CA.pem\n",
            "Adding debian:emSign_Root_CA_-_G1.pem\n",
            "Adding debian:D-TRUST_BR_Root_CA_1_2020.pem\n",
            "Adding debian:IdenTrust_Commercial_Root_CA_1.pem\n",
            "Adding debian:SSL.com_Root_Certification_Authority_ECC.pem\n",
            "Adding debian:DigiCert_TLS_RSA4096_Root_G5.pem\n",
            "Adding debian:AC_RAIZ_FNMT-RCM_SERVIDORES_SEGUROS.pem\n",
            "Adding debian:vTrus_ECC_Root_CA.pem\n",
            "Adding debian:HARICA_TLS_RSA_Root_CA_2021.pem\n",
            "Adding debian:SZAFIR_ROOT_CA2.pem\n",
            "Adding debian:UCA_Global_G2_Root.pem\n",
            "Adding debian:AffirmTrust_Premium_ECC.pem\n",
            "Adding debian:DigiCert_Global_Root_G3.pem\n",
            "Adding debian:ANF_Secure_Server_Root_CA.pem\n",
            "Adding debian:D-TRUST_Root_Class_3_CA_2_EV_2009.pem\n",
            "Adding debian:Trustwave_Global_ECC_P384_Certification_Authority.pem\n",
            "Adding debian:GlobalSign_Root_CA_-_R6.pem\n",
            "Adding debian:Amazon_Root_CA_3.pem\n",
            "Adding debian:DigiCert_Global_Root_G2.pem\n",
            "Adding debian:Security_Communication_RootCA2.pem\n",
            "Adding debian:Baltimore_CyberTrust_Root.pem\n",
            "Adding debian:Certainly_Root_R1.pem\n",
            "Adding debian:GlobalSign_Root_CA_-_R3.pem\n",
            "Adding debian:Security_Communication_RootCA3.pem\n",
            "Adding debian:GTS_Root_R1.pem\n",
            "Adding debian:certSIGN_ROOT_CA.pem\n",
            "Adding debian:HARICA_TLS_ECC_Root_CA_2021.pem\n",
            "Adding debian:Microsoft_ECC_Root_Certificate_Authority_2017.pem\n",
            "Adding debian:GlobalSign_ECC_Root_CA_-_R5.pem\n",
            "Adding debian:Entrust_Root_Certification_Authority_-_G2.pem\n",
            "Adding debian:DigiCert_Trusted_Root_G4.pem\n",
            "Adding debian:QuoVadis_Root_CA_3.pem\n",
            "Adding debian:GlobalSign_Root_R46.pem\n",
            "Adding debian:Starfield_Class_2_CA.pem\n",
            "Adding debian:AffirmTrust_Networking.pem\n",
            "Adding debian:emSign_ECC_Root_CA_-_G3.pem\n",
            "Adding debian:Certigna.pem\n",
            "Adding debian:CommScope_Public_Trust_RSA_Root-02.pem\n",
            "Adding debian:CommScope_Public_Trust_ECC_Root-02.pem\n",
            "Adding debian:BJCA_Global_Root_CA1.pem\n",
            "Adding debian:BJCA_Global_Root_CA2.pem\n",
            "Adding debian:Sectigo_Public_Server_Authentication_Root_R46.pem\n",
            "Adding debian:TrustAsia_Global_Root_CA_G3.pem\n",
            "Adding debian:Atos_TrustedRoot_Root_CA_ECC_TLS_2021.pem\n",
            "Adding debian:Atos_TrustedRoot_Root_CA_RSA_TLS_2021.pem\n",
            "Adding debian:CommScope_Public_Trust_ECC_Root-01.pem\n",
            "Adding debian:Sectigo_Public_Server_Authentication_Root_E46.pem\n",
            "Adding debian:SSL.com_TLS_RSA_Root_CA_2022.pem\n",
            "Adding debian:CommScope_Public_Trust_RSA_Root-01.pem\n",
            "Adding debian:TrustAsia_Global_Root_CA_G4.pem\n",
            "Adding debian:SSL.com_TLS_ECC_Root_CA_2022.pem\n",
            "done.\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for ca-certificates (20240203~22.04.1) ...\n",
            "Updating certificates in /etc/ssl/certs...\n",
            "0 added, 0 removed; done.\n",
            "Running hooks in /etc/ca-certificates/update.d...\n",
            "\n",
            "done.\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] += \":/usr/lib/jvm/java-11-openjdk-amd64/bin\""
      ],
      "metadata": {
        "id": "1D_6npvl5fNc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "hVPpBbZG3_Xe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "nRbr7hp54Myz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SparkContext object y Spark Session"
      ],
      "metadata": {
        "id": "tHWfDsG64Xk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark DataFrames basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "oigMAPpz4iBb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargar conjuntos de datos en DataFrames de PySpark"
      ],
      "metadata": {
        "id": "Fpxn1cob55Jm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset1.csv\n",
        "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset2.csv\n"
      ],
      "metadata": {
        "id": "O9VBpYmz6Hg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "\n",
        "link_to_data1 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset1.csv'\n",
        "wget.download(link_to_data1)\n",
        "\n",
        "link_to_data2 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset2.csv'\n",
        "wget.download(link_to_data2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HXuxFuBc6G2z",
        "outputId": "b6fb3568-7ed5-47c4-9ccb-511517363254"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dataset2.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargar los datos en un Dataframe"
      ],
      "metadata": {
        "id": "Gx3T6onj6soa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.read.csv(\"dataset1.csv\", header=True, inferSchema=True)\n",
        "df2 = spark.read.csv(\"dataset2.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "wrhaRLn857un"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Esquema de df1 y df2 para comprender la estructura de los conjuntos de datos."
      ],
      "metadata": {
        "id": "v00A9E5q69yD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.printSchema()\n",
        "df2.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js14GE_17JJI",
        "outputId": "38eab0a3-a488-4aad-9147-25a52192be9f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- date_column: string (nullable = true)\n",
            " |-- amount: integer (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- transaction_date: string (nullable = true)\n",
            " |-- value: integer (nullable = true)\n",
            " |-- notes: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agregando una nueva columna a cada dataframe"
      ],
      "metadata": {
        "id": "LToHmQv47d7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agregamos una nueva columna llamada **año** a `df1` y **trimestre** a `df2` que represente el año y el trimestre de los datos."
      ],
      "metadata": {
        "id": "4WNpQB6f7p7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import year, quarter, to_date\n",
        "\n",
        "# Agregar nueva columna 'year' a df1\n",
        "df1 = df1.withColumn('year', year(to_date('date_column', 'dd/MM/yyyy')))\n",
        "\n",
        "# Agregar nueva columna 'quarter' a df2\n",
        "df2 = df2.withColumn('quarter', quarter(to_date('transaction_date', 'dd/MM/yyyy')))\n"
      ],
      "metadata": {
        "id": "4xm8U9yb7hZK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cambiamos el nombre de la columna amount a transaction_amount en df1 y value a transaction_value en df2."
      ],
      "metadata": {
        "id": "7KLB_pOG8tU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Rename df1 column amount to transaction_amount\n",
        "df1 = df1.withColumnRenamed('amount', 'transaction_amount')\n",
        "\n",
        "#Rename df2 column value to transaction_value\n",
        "df2 = df2.withColumnRenamed('value', 'transaction_value')"
      ],
      "metadata": {
        "id": "RqYI8uyC8kr6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eliminar columnas innecesarias\n",
        " Eliminar la descripción y ubicación de las columnas de df1 y las notas de df2."
      ],
      "metadata": {
        "id": "xSBiCXs09MsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df1 = df1.drop('description', 'location')\n",
        "\n",
        "\n",
        "df2 = df2.drop('notes')"
      ],
      "metadata": {
        "id": "RckWdqyV9RqS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unir dataframes basándose en una columna común\n",
        "\n",
        "Unir df1 y df2 basándose en la columna común customer_id y crear un nuevo dataframe llamado joined_df."
      ],
      "metadata": {
        "id": "4RxXng0t9shO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df = df1.join(df2, 'customer_id', 'inner')"
      ],
      "metadata": {
        "id": "nR2UTu3c9vFK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtrar datos según una condición\n",
        "Filtrar joined_df para incluir solo las transacciones donde \"transaction_amount\" sea mayor que 1000 y crear un nuevo dataframe llamado filtered_df."
      ],
      "metadata": {
        "id": "6UjWGv-t-B9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df = joined_df.filter(\"transaction_amount > 1000\")"
      ],
      "metadata": {
        "id": "ldOB3TSJ-EJv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agrega datos por cliente\n",
        " Calcula el importe total de las transacciones para cada cliente en filtered_df y muestra el resultado."
      ],
      "metadata": {
        "id": "gls2I5Ud-QoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum\n",
        "\n",
        "\n",
        "total_amount_per_customer = filtered_df.groupBy('customer_id').agg(sum('transaction_amount').alias('total_amount'))\n",
        "\n",
        "\n",
        "total_amount_per_customer.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz-gjfnS-VtW",
        "outputId": "ab7feebc-c1e3-4bc6-ca4e-0537de604128"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+\n",
            "|customer_id|total_amount|\n",
            "+-----------+------------+\n",
            "|         31|        3200|\n",
            "|         85|        1800|\n",
            "|         78|        1500|\n",
            "|         34|        1200|\n",
            "|         81|        5500|\n",
            "|         28|        2600|\n",
            "|         76|        2600|\n",
            "|         27|        4200|\n",
            "|         91|        3200|\n",
            "|         22|        1200|\n",
            "|         93|        5500|\n",
            "|          1|        5000|\n",
            "|         52|        2600|\n",
            "|         13|        4800|\n",
            "|          6|        4500|\n",
            "|         16|        2600|\n",
            "|         40|        2600|\n",
            "|         94|        1200|\n",
            "|         57|        5500|\n",
            "|         54|        1500|\n",
            "+-----------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Escribimos el resultado en una tabla de Hive\n",
        "Escribimos total_amount_per_customer en una tabla de Hive llamada customer_totals."
      ],
      "metadata": {
        "id": "h0c_1Ia2-u-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_amount_per_customer.write.mode(\"overwrite\").saveAsTable(\"customer_totals\")"
      ],
      "metadata": {
        "id": "9hD9wSLf-0JW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Escribimos los datos filtrados en HDFS\n",
        "\n",
        "Escribimos filtered_df en HDFS en formato Parquet en un archivo llamado filtered_data."
      ],
      "metadata": {
        "id": "90ojXsMN_HfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
        "\n",
        "filtered_df.write.mode(\"overwrite\").parquet(\"filtered_data.parquet\")"
      ],
      "metadata": {
        "id": "mxNKUv7v_PDm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agregamos una nueva columna según una condición\n",
        "\n",
        "Agregar una nueva columna llamada high_value a df1 que indique si transaction_amount es mayor que 5000. Cuando el valor sea mayor que 5000, el valor de la columna debe ser Sí. Cuando el valor sea menor o igual a 5000, el valor de la columna debe ser No."
      ],
      "metadata": {
        "id": "nQWj5P1rAMYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, lit\n",
        "\n",
        "df1 = df1.withColumn(\"high_value\", when(df1.transaction_amount > 5000, lit(\"Yes\")).otherwise(lit(\"No\")))"
      ],
      "metadata": {
        "id": "XBF7sPVgAQyl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculamos el valor promedio de las transacciones por trimestre\n",
        "\n",
        " Calcular y mostrar el valor promedio de las transacciones para cada trimestre en df2 y crear un nuevo dataframe llamado average_value_per_quarter con la columna avg_trans_val."
      ],
      "metadata": {
        "id": "hyi_S3Q6AiZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "average_value_per_quarter = df2.groupBy('quarter').agg(avg(\"transaction_value\").alias(\"avg_trans_val\"))\n",
        "\n",
        "average_value_per_quarter.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS6pL9zXAnms",
        "outputId": "eb47c67e-d8ac-4c12-8a5e-cfac012e145d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+\n",
            "|quarter|     avg_trans_val|\n",
            "+-------+------------------+\n",
            "|      1| 1111.111111111111|\n",
            "|      3|1958.3333333333333|\n",
            "|      4| 816.6666666666666|\n",
            "|      2|            1072.0|\n",
            "+-------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Escribimos el resultado en una tabla de Hive\n",
        "\n",
        "Escribimos average_value_per_quarter en una tabla de Hive llamada quarterly_averages."
      ],
      "metadata": {
        "id": "yAYTV8k8A9T6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "average_value_per_quarter.write.mode(\"overwrite\").saveAsTable(\"quarterly_averages\")"
      ],
      "metadata": {
        "id": "mDWMxEJWBC0k"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculamos el valor total de las transacciones por año\n",
        "\n",
        " Calcular y mostrar el valor total de las transacciones para cada año en df1 y crear un nuevo dataframe llamado total_value_per_year con la columna total_transaction_val."
      ],
      "metadata": {
        "id": "cnlT9k4PBSDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_value_per_year = df1.groupBy('year').agg(sum(\"transaction_amount\").alias(\"total_transaction_val\"))\n",
        "\n",
        "total_value_per_year.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BtoOFYmBZKz",
        "outputId": "dfec7969-3d64-4947-fa6b-3771f0ba23dc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------------+\n",
            "|year|total_transaction_val|\n",
            "+----+---------------------+\n",
            "|2025|                25700|\n",
            "|2027|                25700|\n",
            "|2023|                28100|\n",
            "|2022|                29800|\n",
            "|2026|                25700|\n",
            "|2029|                25700|\n",
            "|2030|                 9500|\n",
            "|2028|                25700|\n",
            "|2024|                25700|\n",
            "+----+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Escribimos el resultado en HDFS\n",
        "\n",
        "Escribimos total_value_per_year en HDFS en formato CSV en un archivo llamado total_value_per_year."
      ],
      "metadata": {
        "id": "1ZYfUxwrBsi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_value_per_year.write.mode(\"overwrite\").csv(\"total_value_per_year.csv\")"
      ],
      "metadata": {
        "id": "JM_-mHmOBxED"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}